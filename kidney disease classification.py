# -*- coding: utf-8 -*-
"""Untitled4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UdUnTjSElR5--PIQER6g2neRLrpwkeuP
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings("ignore")

df = pd.read_csv("/content/kidney_disease.csv")

df.head()

df.tail()

df.info()

df.head()

df_new = pd.DataFrame(df)
df_new.head()

df_newty = df_new.drop(['id'], axis=1)
df_newty.head()

df_newty.columns = [
    'age', 'blood_pressure', 'specific_gravity', 'albumin', 'sugar', 'red_blood_cells', 'pus_cell',"pus cell clumps",
    "bacteria", "blood glucose random", "blood_urea", "serum_creatinine", "sodium", "potassium", "hemoglobin",
    "packed_cell_volume", "white_blood_cell_count", "red_blood_cell_count", "hypertension", "diabetes_mellitus",
    "coronary_artery_disease", "appetite", "pedal_edema", "anemia", "class"
]

df_newty.head()

df_newty[['specific_gravity', 'albumin', 'sugar']] = df_newty[['specific_gravity', 'albumin', 'sugar']].astype('object')

df_newty["packed_cell_volume"] = pd.to_numeric(df_newty["packed_cell_volume"],errors = "coerce")
df_newty['white_blood_cell_count'] = pd.to_numeric(df_newty['white_blood_cell_count'],errors = "coerce")
df_newty["red_blood_cell_count"] = pd.to_numeric(df_newty['red_blood_cell_count'],errors = "coerce")

# extracting categerical and numerical columns
cat_col = [col for col in df_newty.columns if df_newty[col].dtype == 'object']
num_col = [col for col in df_newty.columns if df_newty[col].dtype != 'object']

cat_col

num_col

# unique values in categorical columns

for col in cat_col:
  print(f'{col} has {df_newty[col].unique()} values')

# replace incorrect values like '\tno','\tyes','yes','\tno','ckd\t','notckd' in categorical cols
df_newty['diabetes_mellitus'].replace({' yes':'yes','\tno':'no','\yes':'yes'},inplace=True)
df_newty['coronary_artery_disease'].replace({'\tno':'no'},inplace=True)
df_newty['class'].replace({'ckd\t':'ckd'},inplace=True)

# converting target col class into 0(chronic kidney) and 1(not a chronic kidney)
df_newty['class'] = df_newty['class'].map({'ckd':0,'notckd':1})

# coverting target col into numeric to check correlation

df_newty['class'] = pd.to_numeric(df_newty['class'],errors = 'coerce')

df_newty.head()

# lets see the cols in numerical col list
num_col

"""Here starts EDA  -  Exploratory Data Analysis"""

# checking numerical features distribution
plt.figure(figsize=(20,12))

#looping over num_col and checking its distribution
for col in num_col:
  sns.displot(df_newty[col],kind='kde')

cat_col

# checking cat features distribution
#create the figure and axes

fig, axes = plt.subplots(3,5,figsize=(16,8))

axes = axes.ravel() # flattening the array makes indexing easier

# loop over cat cols and plot countplot
for col,ax in zip(cat_col,axes):
  sns.countplot(data = df_newty,x = col, ax = ax)
  plt.xlabel(col)
  fig.tight_layout()

# correlated heatmap of data
plt.figure(figsize = (15,8))
sns.heatmap(df_newty.corr(numeric_only = True),annot = True,linewidths = 2,linecolor = 'lightgrey')
plt.show()

# let's check count of null values in whole df

df_newty.isnull().sum().sort_values(ascending = False)

# let's check of null values in num_col

df_newty[num_col].isnull().sum().sort_values(ascending = False)

# let's check count of null values in cat_col
df_newty[cat_col].isnull().sum().sort_values(ascending = False)

# filling null values, we will use two methods, random sampling for higher null values and
# mean / mode sampling for lower null values

# creating function for importing random values
def random_value_imputing(feature):
  random_sample = df_newty[feature].dropna().sample(df_newty[feature].isna().sum())
  random_sample.index = df_newty[df_newty[feature].isnull()].index
  df_newty.loc[df_newty[feature].isnull(), feature] = random_sample

# creating function for imputing most common value(modal value)
def impute_mode(feature):
  mode = df_newty[feature].mode()[0]
  df_newty[feature] = df_newty[feature].fillna(mode)

# filling num_col null values using random sampling method
for col in num_col:
  random_value_imputing(col)

# let's check count of null value in num_col again
df_newty[num_col].isnull().sum().sort_values(ascending = False)

# filling "red_blood_cells" and 'pus_cell' using random sampling method and rest of cat_cols using mode imputation
random_value_imputing('red_blood_cells')
random_value_imputing('pus_cell')

# let's check count of null values in cat_col again
df_newty[cat_col].isnull().sum().sort_values(ascending = False)

# filling cat null values with mode treatment
for col in cat_col:
  impute_mode(col)

df_newty.head()

# checking unique values in each cat_col by looping over cat_col
for col in cat_col:
  print(f'{col} has {df_newty[col].nunique()} categories')

# using labelencoder and applying on cat_col
from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
for col in cat_col[3:]:
  df_newty[col] = le.fit_transform(df_newty[col])

# check df_newty after transforming cat_col
df_newty.head()

# split data into features and target variables (x and y)

x = df_newty.drop('class',axis = 1)
y = df_newty['class']

# spliting data into training and test set, so import train_test_split

from sklearn.model_selection import train_test_split

x_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.30, random_state = 0)

"""Model Building"""

# import KNeighborsClassifier, accuracy_score, confusion_matrix, Classification_report

from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

knn = KNeighborsClassifier()
knn.fit(x_train,y_train)

# accuracy score, confusion matrix and classification report of knn

knn_acc = accuracy_score(y_test,knn.predict(x_test))

print(f'training accuracy of knn is {accuracy_score(y_train, knn.predict(x_train))}')
print(f'testing accuracy of knn is {knn_acc} \n')

print(f"confusion matrix : - \n{confusion_matrix(y_test, knn.predict(x_test))}\n")
print(f"classification report : - \n {classification_report(y_test, knn.predict(x_test))}")

# import Decisiontreeclassifier
from sklearn.tree import DecisionTreeClassifier

dtc = DecisionTreeClassifier()
dtc.fit(x_train, y_train)

# accuracy score, confusion matrix and classification report of decision tree

dtc_acc = accuracy_score(y_test, dtc.predict(x_test))
print(dtc_acc)

print(f"training accuracy of dtc is {accuracy_score(y_train,dtc.predict(x_train))}")
print(f"training accuracy of dfc is {accuracy_score(y_test,dtc.predict(x_test))}")

print(f'confusion matrix : - {confusion_matrix(y_test,dtc.predict(x_test))}')
print(f"classification report : - {classification_report(y_test,dtc.predict(x_test))}")

# hyper parameter tuning of decision tree , import gridsearchcv

from sklearn.model_selection import GridSearchCV
grid_param = {
    'criterion' : ['gini','entropy'],
    'max_depth' : [3, 5, 7, 10],
    'min_samples_leaf' : [1,2,3,5,7],
    'min_samples_split' : [1,2,3,5,7],
    'splitter' : ['best','random'],
    'max_features' : ['auto', 'sqrt','log2']
}
# apply gridsearchcv with cv = 5, n_jobs = -1 verbose = 1

grid_search_dtc = GridSearchCV(dtc,grid_param,cv = 5,n_jobs = -1,verbose = 1)
grid_search_dtc = grid_search_dtc.fit(x_train,y_train)

# print best parameters and best score in grid search dtc

print(grid_search_dtc.best_params_)
print(grid_search_dtc.best_score_)

# storing best estimator

dtc = grid_search_dtc.best_estimator_

# accuracy score, confusion matrix and classification report of decision tree

dtc_acc = accuracy_score(y_test, dtc.predict(x_test))

print(f'training accuracy of decision tree classifier is {accuracy_score(y_train, dtc.predict(x_train))}')
print(f'testing accuracy of decision tree classifier is {dtc_acc} \n')

print(f'confusion matrix : - {confusion_matrix(y_test, dtc.predict(x_test))}\n')
print(f'classification report : - {classification_report(y_test,dtc.predict(x_test))}')

#import randomforestclassifier

from sklearn.ensemble import RandomForestClassifier

rd_clf = RandomForestClassifier(criterion = 'entropy', max_depth = 11, max_features = 'sqrt', min_samples_leaf = 2, min_samples_split = 3, n_estimators = 130)

rd_clf.fit(x_train, y_train)



#accuracy score, confusion matrix and classification report of random forest

rf_acc = accuracy_score(y_test, rd_clf.predict(x_test))

print(f'training accuracy of random forest classifier is {accuracy_score(y_train, rd_clf.predict(x_train))}')
print(f'testing accuracy of random forest classifier is {rf_acc}\n')

print(f'confusion matrix : - {confusion_matrix(y_test, rd_clf.predict(x_test))}\n')
print(f'classification report : - {classification_report(y_test,rd_clf.predict(x_test))}')

# import adaboostclassifier

from sklearn.ensemble import AdaBoostClassifier

ada_clf = AdaBoostClassifier(estimator=dtc)
ada_clf.fit(x_train, y_train)

# accuracy score, confusion matrix and classification report of adaboost

ada_acc = accuracy_score(y_test, ada_clf.predict(x_test))

print(f'training accuracy of adc is {accuracy_score(y_train, ada_clf.predict(x_train))}')
print(f'testing accuracy of adaboost classifier is {knn_acc}\n')

print(f'confusion matrix : - {confusion_matrix(y_test, ada_clf.predict(x_test))}\n')
print(f'classification report : - {classification_report(y_test,ada_clf.predict(x_test))}')

# import gradientboostingclassifier

from sklearn.ensemble import GradientBoostingClassifier

gb_clf = GradientBoostingClassifier()
gb_clf.fit(x_train, y_train)

# accuracy score, confusion matrix and classification report of gradient boosting classifier

gb_acc = accuracy_score(y_test, gb_clf.predict(x_test))

print(f'training accuracy of gradient boosting classifier is {accuracy_score(y_train, gb_clf.predict(x_train))}')
print(f'testing accuracy of gradient boosting classifier is {knn_acc}\n')

print(f'confusion matrix : - {confusion_matrix(y_test, gb_clf.predict(x_test))}\n')
print(f'classification report : - {classification_report(y_test,gb_clf.predict(x_test))}')

#using max_depth = 4, subsample = 0.90, max_features = 0.75, n_estimators = 200

sgb_clf = GradientBoostingClassifier(max_depth = 4, subsample = 0.90, max_features = 0.75, n_estimators = 200)
sgb_clf.fit(x_train, y_train)

#accuracy score, confusion matrix and classification report of stochastic gradient boosting classifier
gb_acc = accuracy_score(y_test, sgb_clf.predict(x_test))

print(f'training accuracy of stochastic gradient boosting classifier is {accuracy_score(y_train, sgb_clf.predict(x_train))}')
print(f'testing accuracy of stochastic gradient boosting classifier is {knn_acc}\n')

print(f'confusion matrix : - {confusion_matrix(y_test, sgb_clf.predict(x_test))}\n')
print(f'classification report : - {classification_report(y_test,sgb_clf.predict(x_test))}')

# import xgbclassifier
from xgboost import XGBClassifier

xgb_clf = XGBClassifier(objetive = 'binary:logistic', learning_rate = 0.5, max_depth = 5, n_estimators = 150)
xgb_clf.fit(x_train, y_train)

# accuracy score, confusion matrix and classification report of xgboost

xgb_acc = accuracy_score(y_test, xgb_clf.predict(x_test))

print(f'training accuracy of xgboost classifier is {accuracy_score(y_train, xgb_clf.predict(x_train))}')
print(f'testing accuracy of xgboost classifier is {knn_acc}\n')

print(f'confusion matrix : - {confusion_matrix(y_test, xgb_clf.predict(x_test))}\n')
print(f'classification report : - {classification_report(y_test,xgb_clf.predict(x_test))}')

pip install catboost

# import catboostclassifier
from catboost import CatBoostClassifier

cat_c = CatBoostClassifier(iterations = 10)
cat_c.fit(x_train, y_train)

#accuracy score, confusion matrix and classification report of cat_c boost

cat_acc = accuracy_score(y_test, cat_c.predict(x_test))

print(f'training accuracy of catboost classifier is {accuracy_score(y_train, cat_c.predict(x_train))}')
print(f'testing accuracy of catboost classifier is {knn_acc}\n')

print(f'confusion matrix : - {confusion_matrix(y_test, cat_c.predict(x_test))}\n')
print(f'classification report : - {classification_report(y_test,cat_c.predict(x_test))}')

# import extratreesclassifier
from sklearn.ensemble import ExtraTreesClassifier

etc_clf = ExtraTreesClassifier()
etc_clf.fit(x_train, y_train)

# accuracy score, confusion matrix and classification report of extra trees classifier

etc_acc = accuracy_score(y_test, etc_clf.predict(x_test))

print(f'training accuracy of extra trees classifier is {accuracy_score(y_train, etc_clf.predict(x_train))}')
print(f'testing accuracy of extra trees classifier is {knn_acc}\n')

print(f'confusion matrix : - {confusion_matrix(y_test, etc_clf.predict(x_test))}\n')
print(f'classification report : - {classification_report(y_test,etc_clf.predict(x_test))}')

# import LGBMClassifier
from lightgbm import LGBMClassifier

lgbm_clf = LGBMClassifier(learnin_rate = 1)
lgbm_clf.fit(x_train, y_train)

# accuracy score, confusion matrix and classification report of LGBM classifier
etc_acc = accuracy_score(y_test, lgbm_clf.predict(x_test))

print(f'training accuracy of LGBM classifier is {accuracy_score(y_train, lgbm_clf.predict(x_train))}')
print(f'testing accuracy of LGBM classifier is {knn_acc}\n')

print(f'confusion matrix : - {confusion_matrix(y_test, lgbm_clf.predict(x_test))}\n')
print(f'classification report : - {classification_report(y_test,lgbm_clf.predict(x_test))}')

# comparing all models accuracy by creating a df
models = pd.DataFrame({
    'model' : ['KNN','Decision tree classifier','Random Forest Classifier','Ada Boost Classifier','Gradient Boosting Classifier','XGBoost Classifier','CatBoost Classifier','Extra Trees Classifier'],
    'score' : [knn_acc,dtc_acc,rf_acc,ada_acc,gb_acc,xgb_acc,cat_acc,etc_acc,]
})

# sort values by score
models.sort_values(by = 'score',ascending = False)







